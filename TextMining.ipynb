{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/tannertamondong/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#First you need to install nltk from\n",
    "#        https://www.nltk.org/install.html   \n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from zipfile import ZipFile\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import nltk\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem.snowball import EnglishStemmer \n",
    "import matplotlib.pylab as plt\n",
    "from dmba import printTermDocumentMatrix, classificationSummary, liftChart\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CountVectorizer' object has no attribute 'get_feature_names'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 14\u001b[0m\n\u001b[1;32m     10\u001b[0m vects \u001b[38;5;241m=\u001b[39m vect\u001b[38;5;241m.\u001b[39mfit_transform(text)\n\u001b[1;32m     13\u001b[0m td \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(vects\u001b[38;5;241m.\u001b[39mtodense())\n\u001b[0;32m---> 14\u001b[0m td\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m vect\u001b[38;5;241m.\u001b[39mget_feature_names()\n\u001b[1;32m     15\u001b[0m term_document_matrix \u001b[38;5;241m=\u001b[39m td\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m     16\u001b[0m term_document_matrix\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSentence \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, td\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'CountVectorizer' object has no attribute 'get_feature_names'"
     ]
    }
   ],
   "source": [
    "# Activate CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "text = ['this is the first sentence.',\n",
    "        'this is a second sentence.',\n",
    "        'the third sentence is here.']\n",
    "\n",
    "# Count Vectorizer\n",
    "vect = CountVectorizer()  \n",
    "vects = vect.fit_transform(text)\n",
    "\n",
    "\n",
    "td = pd.DataFrame(vects.todense())\n",
    "td.columns = vect.get_feature_names()\n",
    "term_document_matrix = td.T\n",
    "term_document_matrix.columns = ['Sentence '+str(i) for i in range(1, td.shape[0]+1)]\n",
    "term_document_matrix['total_count'] = term_document_matrix.sum(axis=1)\n",
    "\n",
    "\n",
    "print(term_document_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = ['this is the first     sentence!!',\n",
    "        'this is a second Sentence :)',\n",
    "        'the third sentence, is here ',\n",
    "        'forth of all sentences']\n",
    "\n",
    "# Learn features based on text. \n",
    "count_vect = CountVectorizer()\n",
    "counts = count_vect.fit_transform(text)\n",
    "\n",
    "\n",
    "# Select the first five rows from the data set\n",
    "td = pd.DataFrame(counts.todense())\n",
    "td.columns = count_vect.get_feature_names()\n",
    "term_document_matrix = td.T\n",
    "term_document_matrix.columns = ['Sentence '+str(i) for i in range(1, td.shape[0]+1)]\n",
    "term_document_matrix['total_count'] = term_document_matrix.sum(axis=1)\n",
    "\n",
    "\n",
    "print(term_document_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "td"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = ['this is the first     sentence!!',\n",
    "        'this is a second Sentence :)',\n",
    "        'the third sentence, is here ',\n",
    "        'forth of all sentences']\n",
    "\n",
    "# Learn features based on text. Include special characters that are part of a word in the analysis\n",
    "count_vect = CountVectorizer(token_pattern='[a-zA-Z!:)]+')\n",
    "counts = count_vect.fit_transform(text)\n",
    "\n",
    "td = pd.DataFrame(counts.todense())\n",
    "td.columns = count_vect.get_feature_names()\n",
    "term_document_matrix = td.T\n",
    "term_document_matrix.columns = ['Sentence '+str(i) for i in range(1, td.shape[0]+1)]\n",
    "term_document_matrix['total_count'] = term_document_matrix.sum(axis=1)\n",
    "\n",
    "\n",
    "print(term_document_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stopWords = list(sorted(ENGLISH_STOP_WORDS))\n",
    "ncolumns = 10; nrows= 2\n",
    "\n",
    "print('First {} of {} stopwords'.format(ncolumns * nrows, len(stopWords)))\n",
    "for i in range(0, len(stopWords[:(ncolumns * nrows)]), ncolumns):\n",
    "    print(''.join(word.ljust(13) for word in stopWords[i:(i+ncolumns)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = ['this is the first     sentence!! ',\n",
    "        'this is a second Sentence :)',\n",
    "        'the third sentence, is here ',\n",
    "        'forth of all sentences']\n",
    "\n",
    "# Create a custom tokenizer that will use NLTK for tokenizing and lemmatizing \n",
    "# (removes interpunctuation and stop words)\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.stemmer = EnglishStemmer()\n",
    "        self.stopWords = set(ENGLISH_STOP_WORDS)\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        return [self.stemmer.stem(t) for t in word_tokenize(doc) \n",
    "                if t.isalpha() and t not in self.stopWords]\n",
    "\n",
    "# Learn features based on text\n",
    "count_vect = CountVectorizer(tokenizer=LemmaTokenizer())\n",
    "counts = count_vect.fit_transform(text)\n",
    "\n",
    "td = pd.DataFrame(counts.todense())\n",
    "td.columns = count_vect.get_feature_names()\n",
    "term_document_matrix = td.T\n",
    "term_document_matrix.columns = ['Sentence '+str(i) for i in range(1, td.shape[0]+1)]\n",
    "term_document_matrix['total_count'] = term_document_matrix.sum(axis=1)\n",
    "\n",
    "\n",
    "\n",
    "print(term_document_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = ['this is the first     sentence!!',\n",
    "        'this is a second Sentence :)',\n",
    "        'the third sentence, is here ',\n",
    "        'forth of all sentences']\n",
    "\n",
    "# Apply CountVectorizer and TfidfTransformer sequentially\n",
    "count_vect = CountVectorizer()\n",
    "tfidfTransformer = TfidfTransformer(smooth_idf=False, norm=None)\n",
    "counts = count_vect.fit_transform(text)\n",
    "tfidf = tfidfTransformer.fit_transform(counts)\n",
    "\n",
    "td = pd.DataFrame(tfidf.todense())\n",
    "td.columns = count_vect.get_feature_names()\n",
    "term_document_matrix = td.T\n",
    "term_document_matrix.columns = ['Sentence '+str(i) for i in range(1, td.shape[0]+1)]\n",
    "\n",
    "\n",
    "\n",
    "print(term_document_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This example illustrates a classification taskâ€”to classify Internet discussion posts\n",
    "#as either auto-related or electronics-related.\n",
    "\n",
    "#Step 1: import and label records\n",
    "corpus = []\n",
    "label = []\n",
    "with ZipFile('AutoAndElectronics.zip') as rawData:\n",
    "    for info in rawData.infolist():\n",
    "        if info.is_dir(): \n",
    "            continue\n",
    "        label.append(1 if 'rec.autos' in info.filename else 0)\n",
    "        corpus.append(rawData.read(info))\n",
    "\n",
    "# Step 2: preprocessing (tokenization, stemming, and stopwords)\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.stemmer = EnglishStemmer()\n",
    "        self.stopWords = set(ENGLISH_STOP_WORDS)\n",
    "    def __call__(self, doc):\n",
    "        return [self.stemmer.stem(t) for t in word_tokenize(doc) \n",
    "                if t.isalpha() and t not in self.stopWords]\n",
    "\n",
    "preprocessor = CountVectorizer(tokenizer=LemmaTokenizer(), encoding='latin1')\n",
    "preprocessedText = preprocessor.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocessedText' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Select the first five rows from the data set\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m td \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(preprocessedText\u001b[38;5;241m.\u001b[39mtodense())\n\u001b[1;32m      3\u001b[0m td\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m preprocessor\u001b[38;5;241m.\u001b[39mget_feature_names()\n\u001b[1;32m      4\u001b[0m term_document_matrix \u001b[38;5;241m=\u001b[39m td\u001b[38;5;241m.\u001b[39mT\n",
      "\u001b[0;31mNameError\u001b[0m: name 'preprocessedText' is not defined"
     ]
    }
   ],
   "source": [
    "# Select the first five rows from the data set\n",
    "td = pd.DataFrame(preprocessedText.todense())\n",
    "td.columns = preprocessor.get_feature_names()\n",
    "term_document_matrix = td.T\n",
    "term_document_matrix.columns = ['Sentence '+str(i) for i in range(1, td.shape[0]+1)]\n",
    "term_document_matrix['total_count'] = term_document_matrix.sum(axis=1)\n",
    "\n",
    "#Top 25 words \n",
    "term_document_matrix = term_document_matrix.sort_values(by ='total_count',ascending=False)[:25] \n",
    "\n",
    "# Print the first 10 rows \n",
    "print(term_document_matrix.drop(columns=['total_count']).head(10))\n",
    "\n",
    "print(term_document_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocessedText' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m preprocessedText\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'preprocessedText' is not defined"
     ]
    }
   ],
   "source": [
    "preprocessedText.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Step 3: TF-IDF and latent semantic analysis\n",
    "tfidfTransformer = TfidfTransformer()\n",
    "tfidf = tfidfTransformer.fit_transform(preprocessedText)\n",
    "\n",
    "# Extract 20 concepts using LSA ()\n",
    "svd = TruncatedSVD(20)\n",
    "normalizer = Normalizer(copy=False)\n",
    "lsa = make_pipeline(svd, normalizer)\n",
    "\n",
    "lsa_tfidf = lsa.fit_transform(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix (Accuracy 0.9587)\n",
      "\n",
      "       Prediction\n",
      "Actual   0   1\n",
      "     0 375  14\n",
      "     1  19 392\n"
     ]
    }
   ],
   "source": [
    "# split dataset into 60% training and 40% test set\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(lsa_tfidf, label, test_size=0.4, random_state=0)\n",
    "\n",
    "# run logistic regression model on training\n",
    "logit_reg = LogisticRegression(solver='lbfgs')\n",
    "logit_reg.fit(Xtrain, ytrain)\n",
    "\n",
    "# print confusion matrix and accuracty\n",
    "classificationSummary(ytest, logit_reg.predict(Xtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
